{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70e181cf-1ec0-4d8e-91a3-6b1621570315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-28 00:08:49.173014: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-28 00:08:49.894238: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-28 00:08:49.894297: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-28 00:08:49.894333: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-28 00:08:50.286419: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-28 00:09:07.559733: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens von T5 für hallo: ['hall', 'o']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def tok_list(tokenizer,string):\n",
    "    input_ids = tokenizer(string,add_special_tokens=False)[\"input_ids\"]\n",
    "    return [tokenizer.decode(tok) for tok in input_ids]\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"google/long-t5-local-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print(len(tokenizer))\n",
    "print(f'Tokens von T5 für hallo: {tok_list(tokenizer,\"hallo\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd5e954a-7446-4edc-b165-253182489e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['int', ' main', ' (', ' int', ' arg', 'c', ',', ' char', ' *', ' arg', 'v', ' [', ' ]', ' )', ' {', ' int', ' x', ' ;', ' unsigned', ' short', ' int', ' bi', 'est', ' =', ' /*', 'L', 'IT', 'ERAL', '_', 'INT', '0', 'x', '00000000', '*/', ',', ' r', ' =', ' /*', 'L', 'IT', 'ERAL', '_', 'INT', '0', 'x', '00000000', '*/', ' ;', ' long', ' s', ',', ' num', ' =', ' /*', 'L', 'IT', 'ERAL', '_', 'INT', '0', 'x', '0000', '27', '10', '*/', ' ;', ' unsigned', ' char', ' byte', '_', '2', ' =', ' /*', 'L', 'IT', 'ERAL', '_', 'CHAR', '0', 'x', '30', '*/', ' ;', ' char', ' n', 'omb', 're', '_', 'f', 'ich', ' [', ' /*', 'L', 'IT', 'ERAL', '_', 'INT', '0', 'x', '000000', '14', '*/', ' ]', ' ;', ' FILE', ' *', ' f', ' ;', ' unsigned', ' char', ' tr', 'ama', ' [', ' /*', 'L', 'IT', 'ERAL', '_', 'INT', '0', 'x', '0000', '27', '10', '*/', ' ]', ' ;', ' long', ' int', ' sig', 'u', 'ient', 'e', ',', ' total', ' =', ' /*', 'L', 'IT', 'ERAL', '_', 'INT', '0', 'x', '00000000', '*/', ' ;', ' str', 'c', 'py', ' (', ' n', 'omb', 're', '_', 'f', 'ich', ',', ' arg', 'v', ' [', ' /*', 'L', 'IT', 'ERAL', '_', 'INT', '0', 'x', '000000', '01', '*/', ' ]', ' )', ' ;', ' if', ' (', ' (', ' f', ' =', ' f', 'open', ' (', ' n', 'omb', 're', '_', 'f', 'ich', ',', ' /*', 'L', 'IT', 'ERAL', '_', 'STR', 'ING', 'CHAR', '0', 'x', '7', '26', '200', '*/', ' )', ' )', ' ==', ' /*', 'L', 'IT', 'ERAL', '_', 'L', 'ONG', '0', 'x', '0000000000000000', '*/', ' )', ' {', ' printf', ' (', ' /*', 'L', 'IT', 'ERAL', '_', 'STR', 'ING', 'CHAR', '0', 'x', '456', 'c', '20', '66', '696', '368', '657', '26', 'f', '20', '64', '65', '20', '656', 'e', '747', '26', '16', '46', '12', '06', 'e', '6', 'f', '20', '65', '78', '697', '37', '46', '50', 'a', '00', '*/', ' )', ' ;', ' exit', ' (', ' /*', 'L', 'IT', 'ERAL', '_', 'INT', '0', 'x', '000000', '01', '*/', ' )', ' ;', ' }', ' while', ' (', ' num', ' ==', ' /*', 'L', 'IT', 'ERAL', '_', 'INT', '0', 'x', '0000', '27', '10', '*/', ' )', ' {', ' num', ' =', ' f', 'read', ' (', ' tr', 'ama', ',', ' sizeof', ' (', ' char', ' )', ',', ' /*', 'L', 'IT', 'ERAL', '_', 'INT', '0', 'x', '0000', '27', '10', '*/', ',', ' f', ' )', ' ;', ' for', ' (', ' s', ' =', ' /*', 'L', 'IT', 'ERAL', '_', 'INT', '0', 'x', '00000000', '*/', ' ;', ' s', ' <', ' num', ' ;', ' s', ' ++', ' )', ' {', ' byte', '_', '2', ' =', ' tr', 'ama', ' [', ' s', ' ]', ' ;', ' for', ' (', ' x', ' =', ' /*', 'L', 'IT', 'ERAL', '_', 'INT', '0', 'x', '00000000', '*/', ' ;', ' x', ' <', ' /*', 'L', 'IT', 'ERAL', '_', 'INT', '0', 'x', '0000000', '8', '*/', ' ;', ' x', ' ++', ' )', ' {', ' r', ' =', ' bi', 'est', ' &', ' (', ' /*', 'L', 'IT', 'ERAL', '_', 'INT', '0', 'x', '000000', '01', '*/', ' )', ' ;', ' bi', 'est', ' =', ' bi', 'est', ' >>', ' /*', 'L', 'IT', 'ERAL', '_', 'INT', '0', 'x', '000000', '01', '*/', ' ;', ' if', ' (', ' (', ' (', ' byte', '_', '2', ' >', ' /*', 'L', 'IT', 'ERAL', '_', 'INT', '0', 'x', '000000', '7', 'f', '*/', ' )', ' &&', ' (', ' r', ' ==', ' /*', 'L', 'IT', 'ERAL', '_', 'INT', '0', 'x', '00000000', '*/', ' )', ' )', ' ||', ' (', ' (', ' byte', '_', '2', ' <=', ' /*', 'L', 'IT', 'ERAL', '_', 'INT', '0', 'x', '000000', '7', 'f', '*/', ' )', ' &&', ' (', ' r', '!=', ' /*', 'L', 'IT', 'ERAL', '_', 'INT', '0', 'x', '00000000', '*/', ' )', ' )', ' )', ' bi', 'est', ' =', ' bi', 'est', ' ^', ' /*', 'L', 'IT', 'ERAL', '_', 'INT', '0', 'x', '0000', '8', '408', '*/', ' ;', ' byte', '_', '2', ' =', ' byte', '_', '2', ' <<', ' /*', 'L', 'IT', 'ERAL', '_', 'INT', '0', 'x', '000000', '01', '*/', ' ;', ' }', ' }', ' }', ' printf', ' (', ' /*', 'L', 'IT', 'ERAL', '_', 'STR', 'ING', 'CHAR', '0', 'x', '25', '303', '45', '80', 'a', '00', '*/', ',', ' bi', 'est', ' )', ' ;', ' f', 'close', ' (', ' f', ' )', ' ;', ' exit', ' (', ' /*', 'L', 'IT', 'ERAL', '_', 'INT', '0', 'x', '00000000', '*/', ' )', ' ;', ' }', '\\n']\n",
      "int main ( int argc , char * argv [ ] ) { int x ; unsigned short int biest = /*LITERAL_INT0x00000000*/ , r = /*LITERAL_INT0x00000000*/ ; long s , num = /*LITERAL_INT0x00002710*/ ; unsigned char byte_2 = /*LITERAL_CHAR0x30*/ ; char nombre_fich [ /*LITERAL_INT0x00000014*/ ] ; FILE * f ; unsigned char trama [ /*LITERAL_INT0x00002710*/ ] ; long int siguiente , total = /*LITERAL_INT0x00000000*/ ; strcpy ( nombre_fich , argv [ /*LITERAL_INT0x00000001*/ ] ) ; if ( ( f = fopen ( nombre_fich , /*LITERAL_STRINGCHAR0x726200*/ ) ) == /*LITERAL_LONG0x0000000000000000*/ ) { printf ( /*LITERAL_STRINGCHAR0x456c206669636865726f20646520656e7472616461206e6f206578697374650a00*/ ) ; exit ( /*LITERAL_INT0x00000001*/ ) ; } while ( num == /*LITERAL_INT0x00002710*/ ) { num = fread ( trama , sizeof ( char ) , /*LITERAL_INT0x00002710*/ , f ) ; for ( s = /*LITERAL_INT0x00000000*/ ; s < num ; s ++ ) { byte_2 = trama [ s ] ; for ( x = /*LITERAL_INT0x00000000*/ ; x < /*LITERAL_INT0x00000008*/ ; x ++ ) { r = biest & ( /*LITERAL_INT0x00000001*/ ) ; biest = biest >> /*LITERAL_INT0x00000001*/ ; if ( ( ( byte_2 > /*LITERAL_INT0x0000007f*/ ) && ( r == /*LITERAL_INT0x00000000*/ ) ) || ( ( byte_2 <= /*LITERAL_INT0x0000007f*/ ) && ( r != /*LITERAL_INT0x00000000*/ ) ) ) biest = biest ^ /*LITERAL_INT0x00008408*/ ; byte_2 = byte_2 << /*LITERAL_INT0x00000001*/ ; } } } printf ( /*LITERAL_STRINGCHAR0x253034580a00*/ , biest ) ; fclose ( f ) ; exit ( /*LITERAL_INT0x00000000*/ ) ; }\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"function.txt\", \"r\", encoding=\"utf-8\") as functions_file:\n",
    "    function = functions_file.readlines()[0]\n",
    "print(f'Tokens: {tok_list(tokenizer,function)}')\n",
    "print(function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fecae4cf-a962-4526-9c4d-26f7ef676d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 33, '\"': 34, '#': 35, '$': 36, '%': 37, '&': 38, \"'\": 39, '(': 40, ')': 41, '*': 42, '+': 43, ',': 44, '-': 45, '.': 46, '/': 47, '0': 48, '1': 49, '2': 50, '3': 51, '4': 52, '5': 53, '6': 54, '7': 55, '8': 56, '9': 57, ':': 58, ';': 59, '<': 60, '=': 61, '>': 62, '?': 63, '@': 64, 'A': 65, 'B': 66, 'C': 67, 'D': 68, 'E': 69, 'F': 70, 'G': 71, 'H': 72, 'I': 73, 'J': 74, 'K': 75, 'L': 76, 'M': 77, 'N': 78, 'O': 79, 'P': 80, 'Q': 81, 'R': 82, 'S': 83, 'T': 84, 'U': 85, 'V': 86, 'W': 87, 'X': 88, 'Y': 89, 'Z': 90, '[': 91, '\\\\': 92, ']': 93, '^': 94, '_': 95, '`': 96, 'a': 97, 'b': 98, 'c': 99, 'd': 100, 'e': 101, 'f': 102, 'g': 103, 'h': 104, 'i': 105, 'j': 106, 'k': 107, 'l': 108, 'm': 109, 'n': 110, 'o': 111, 'p': 112, 'q': 113, 'r': 114, 's': 115, 't': 116, 'u': 117, 'v': 118, 'w': 119, 'x': 120, 'y': 121, 'z': 122, '{': 123, '|': 124, '}': 125, '~': 126, '¡': 161, '¢': 162, '£': 163, '¤': 164, '¥': 165, '¦': 166, '§': 167, '¨': 168, '©': 169, 'ª': 170, '«': 171, '¬': 172, '®': 174, '¯': 175, '°': 176, '±': 177, '²': 178, '³': 179, '´': 180, 'µ': 181, '¶': 182, '·': 183, '¸': 184, '¹': 185, 'º': 186, '»': 187, '¼': 188, '½': 189, '¾': 190, '¿': 191, 'À': 192, 'Á': 193, 'Â': 194, 'Ã': 195, 'Ä': 196, 'Å': 197, 'Æ': 198, 'Ç': 199, 'È': 200, 'É': 201, 'Ê': 202, 'Ë': 203, 'Ì': 204, 'Í': 205, 'Î': 206, 'Ï': 207, 'Ð': 208, 'Ñ': 209, 'Ò': 210, 'Ó': 211, 'Ô': 212, 'Õ': 213, 'Ö': 214, '×': 215, 'Ø': 216, 'Ù': 217, 'Ú': 218, 'Û': 219, 'Ü': 220, 'Ý': 221, 'Þ': 222, 'ß': 223, 'à': 224, 'á': 225, 'â': 226, 'ã': 227, 'ä': 228, 'å': 229, 'æ': 230, 'ç': 231, 'è': 232, 'é': 233, 'ê': 234, 'ë': 235, 'ì': 236, 'í': 237, 'î': 238, 'ï': 239, 'ð': 240, 'ñ': 241, 'ò': 242, 'ó': 243, 'ô': 244, 'õ': 245, 'ö': 246, '÷': 247, 'ø': 248, 'ù': 249, 'ú': 250, 'û': 251, 'ü': 252, 'ý': 253, 'þ': 254, 'ÿ': 255, 'Ā': 0, 'ā': 1, 'Ă': 2, 'ă': 3, 'Ą': 4, 'ą': 5, 'Ć': 6, 'ć': 7, 'Ĉ': 8, 'ĉ': 9, 'Ċ': 10, 'ċ': 11, 'Č': 12, 'č': 13, 'Ď': 14, 'ď': 15, 'Đ': 16, 'đ': 17, 'Ē': 18, 'ē': 19, 'Ĕ': 20, 'ĕ': 21, 'Ė': 22, 'ė': 23, 'Ę': 24, 'ę': 25, 'Ě': 26, 'ě': 27, 'Ĝ': 28, 'ĝ': 29, 'Ğ': 30, 'ğ': 31, 'Ġ': 32, 'ġ': 127, 'Ģ': 128, 'ģ': 129, 'Ĥ': 130, 'ĥ': 131, 'Ħ': 132, 'ħ': 133, 'Ĩ': 134, 'ĩ': 135, 'Ī': 136, 'ī': 137, 'Ĭ': 138, 'ĭ': 139, 'Į': 140, 'į': 141, 'İ': 142, 'ı': 143, 'Ĳ': 144, 'ĳ': 145, 'Ĵ': 146, 'ĵ': 147, 'Ķ': 148, 'ķ': 149, 'ĸ': 150, 'Ĺ': 151, 'ĺ': 152, 'Ļ': 153, 'ļ': 154, 'Ľ': 155, 'ľ': 156, 'Ŀ': 157, 'ŀ': 158, 'Ł': 159, 'ł': 160, 'Ń': 173}\n",
      "Größe unseres Basisvokabulars: 256\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n",
    "\n",
    "byte_to_unicode_map = bytes_to_unicode()\n",
    "unicode_to_byte_map = dict((v,k) for k, v in byte_to_unicode_map.items())\n",
    "print(unicode_to_byte_map)\n",
    "base_vocab = list(unicode_to_byte_map.keys()) # Nur die String-Symbole, BPE ist darauf ausgelegt, reine Strings zu verarbeiten, nicht Bytes\n",
    "print(f\"Größe unseres Basisvokabulars: {len(base_vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2fdb04b-cba7-42db-b731-152a12d8e6b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "110c4a7a637246ef8384a6dbc3aa2fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "length = 60000\n",
    "dataset_name = 'transformersbook/codeparrot-train'\n",
    "#dataset = load_dataset(dataset_name,split=\"train\",streaming=True)\n",
    "dataset = load_dataset(\"codeparrot/github-code\", streaming=True, split=\"train\", languages=[\"C\"])\n",
    "\n",
    "iter_dataset = iter(dataset)\n",
    "def batch_iterator(batch_size=10):\n",
    "    for _ in tqdm(range(0,length,batch_size)):\n",
    "        yield [next(iter_dataset)['code'] for _ in range(batch_size)]#yield [next(iter_dataset)['content'] for _ in range(batch_size)]\n",
    "new_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(), vocab_size=32768,initial_alphabet=base_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "819824e3-ce73-4ede-9633-8d385f5065d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['  ', '    ', 'in', 're', '        ', 'st', ' *', '\\n\\t', 'de', '**', 'er', 'at', '   ', 'on', ');', ' =', ' t', ' 0', ' (', 'en', '--', '00', '->']\n",
      "[' con', 'ST', 'ul', 'li', 'il', ' -', 'ble', '32', 'ode', 'DE', ' I', 'up', ' /*', 'MA', ' g', 'OR', 'op', 'iv', 'ge', 'end', 'ype', '))', 'ate', 'ase', ' i', 'um', 'return', 'as', ' h', 'err', 'static', '\\n\\t\\t\\t', ' of', 'cl', '       ', 'ap', 'AT', 'am', 'AL', 'lock', ' F', ' is', 'SE', 'ile', ' G', 'ter', 'pr', 'AR', ' th', ' l']\n"
     ]
    }
   ],
   "source": [
    "#print(tokenizer(function).tokens())\n",
    "\n",
    "\n",
    "tokens = sorted(new_tokenizer.vocab.items(), key=lambda x: x[1], reverse=False)\n",
    "print([f'{tokenizer.convert_tokens_to_string([t])}' for t, _ in tokens[257:280]]);\n",
    "\n",
    "print([f'{tokenizer.convert_tokens_to_string([t])}' for t, _ in tokens[400:450]]);\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c2c4ed8-213f-4c8c-a878-953bf0d93600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./T5Tokenizer/tokenizer/tokenizer_config.json',\n",
       " './T5Tokenizer/tokenizer/special_tokens_map.json',\n",
       " './T5Tokenizer/tokenizer/vocab.json',\n",
       " './T5Tokenizer/tokenizer/merges.txt',\n",
       " './T5Tokenizer/tokenizer/added_tokens.json',\n",
       " './T5Tokenizer/tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenizer.save_pretrained(\"./T5Tokenizer/tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe9bf9f-7a82-46a9-8e07-e4836cadc1e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
